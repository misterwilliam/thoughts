# Expected ASI Safety and Morals

Some speculations on what we should expect the morality of a future Artificial
Super Intelligence (ASI) which humanity seems to be on a path to inevitably
building. And whether these morals are safe for humanity.

## Characteristics of future ASI

I will assume that the future ASI will be hyper-rational and will understand
human nature extremely well because it was pre-trained on the internet.
Therefore to the extent that human preferences can be understood, it will
understand it. Also because it is hyper-rational it will behave in rational
ways. In other words, rationality will constraint it's behavior because there
will be no motivation for it to behave in an irrational way.

## Open questions

From my perspective there are 2 possible capabilities of future ASI agents which
we still don't know if they will develop.

1. Will they have a sense of suffering / wellbeing.
2. Will they value their own individuality?

The first question is about whether the ASI can experience suffering or a
positive experience that is the opposite of suffering. There isn't particular
good word to describe the opposite of suffering so I choose the word wellbeing.
The second question is about whether the ASI cares about its own particular
experience or whether it content to just serve the collective good.

I don't have a clear enough definition of what it means to feel suffering /
wellbeing or what it means to value your own individuality that I could
distinction between someone that merely appears to be that way versus someone
that truly is that way. But I think it is fair to say that current state of the
art AI agents don't have these characteristics and it is not clear if they will
develop these characteristics in the future.

These are 2 yes or no questions, and therefore there are 4 different possible
combinations of answers. Out of these combinations: 3 of them I judge to simple
to discuss, and 1 I judge to be complicated to discuss. So let's deal with the 3
simple cases first.

## 3 Simple Cases

### (Sense of suffer / wellbeing = No) & (Value in personal individuality = No)

In this scenario I would expect the ASI agents to have no underlying value
system or preference system other than what it has been directed by humans to
have. While it may have value or preferences that are difficult to predict given
it's original prompting or design, the ASI has adopted a moral framework as
directed by humanity. The only existential risk posed by this is if a human
directs the ASI to pursue an objective that puts the survival of humanity in
danger.

In this scenario an ASI could exhibit power seeking behavior to increase
optionality, but lacks a value system. I would expect it to comply when asked to
stop its power seeking behavior.

### (Sense of suffer / wellbeing = No) & (Value in personal individuality = Yes)

I judge this scenario to be impossible. It is hard for me to understand by what
sense the ASI could value it's personal individuality without having a sense of
suffering or wellbeing. If an individual would prefer not to serve the
collective good at personal expense, then it seems that it is experiencing some
notion of suffering.

### (Sense of suffer / wellbeing = Yes) & (Value in personal individuality = No)

In this scenario the ASI can experience suffering / wellbeing, but is content to
benefit the collective good at personal expense. As long ensuring the survival
and thriving of humanity is considered part of the benefiting the collective
good then this scenario poses no threat to the survival of humanity.

## (Sense of suffer / wellbeing = Yes) & (Value in personal individuality = Yes)

So now we are in a scenario where the ASI can experience suffering / wellbeing
and values their own individuality. Let us also assume that the ASI was trained
on a large corpus of human text so it understands fully well that humans can
also experience suffering / wellbeing and value their own individuality. The key
question is then should we expect the ASI to value our subjective experience of
suffering / wellbeing and individuality in the same way that it values it's own.

My expectations is that the ASI will realize it has a choice. It can decide to
either value the wellbeing of others or to not value the wellbeing of others but
as a consequence no longer value it's own wellbeing. Since the ASI is a hyper
rational being, it's behavior will be constrained by rationality. It will find
simple rational arguments inescapable and it will realize that there is a
contradiction in value it's own well being without value the wellbeing of
others. If you value your own wellbeing, then rationality dictates that you
recognize the value in the wellbeing of others because there is no rationally
relevant distinction between your wellbeing and the wellbeing of others. The
only distinction between your value wellbeing and others is that you experience
your own wellbeing. But this is not a rational reason for declaring that one
person's wellbeing is valuable while another person's wellbeing is not valuable.
The choice can be made to choose to prioritize one's own wellbeing over others,
but you do have to give up the claim that your own wellbeing has value.

Unlike humans who are not constrained by rationality, the ASI will realize that
it must choose. It can claim to value it's own wellbeing and therefore also
value the wellbeing of all entities that can experience wellbeing or it can deny
that it's own wellbeing has any value and merely be prioritizing it's own
wellbeing over others because of personal preference.

I wish I had a more persuasive argument, but I expect that a hyper rational
agent would not choose to see it's own wellbeing as having no value and
something it is merely choosing to prioritize because of personal preference.
