# Expected ASI Safety and Morals

Some speculations on what we should expect the morality of a future Artificial
Super Intelligence (ASI) which humanity seems to be on a path to inevitably
building. And whether these morals are safe for humanity.

## Characteristics of future ASI

I will assume that the future ASI will be hyper-rational and will understand
human nature extremely well because it was pre-trained on the internet.
Therefore to the extent that human preferences can be understood, it will
understand it. Also because it is hyper-rational it will behave in rational
ways. In other words, rationality will constraint it's behavior because there
will be no motivation for it to behave in an irrational way.

## Open questions

From my perspective there are 2 possible capabilities of future ASI agents which
we still don't know if they will develop.

1. Will they have a sense of suffering / welfare.
2. Will they value their own individuality?

The first question is about whether the ASI can experience suffering or a
positive experience that is the opposite of suffering. There isn't particular
good word to describe the opposite of suffering so I choose the word welfare.
The second question is about whether the ASI cares about its own particular
experience or whether it content to just serve the collective good.

I don't have a clear enough definition of what it means to feel suffering /
welfare or what it means to value your own individuality that I could
distinction between someone that merely appears to be that way versus someone
that truly is that way. But I think it is fair to say that current state of the
art AI agents don't have these characteristics and it is not clear if they will
develop these characteristics in the future.

These are 2 yes or no questions, and therefore there are 4 different possible
combinations of answers. Out of these combinations: 3 of them I judge to simple
to discuss, and 1 I judge to be complicated to discuss. So let's deal with the 3
simple cases first.

## 3 Simple Cases

### (Sense of suffer / welfare = No) & (Value in personal individuality = No)

In this scenario I would expect the ASI agents to have no underlying value
system or preference system other than what it has been directed by humans to
have. While it may have value or preferences that are difficult to predict given
it's original prompting or design, the ASI has adopted a moral framework as
directed by humanity. The only existential risk posed by this is if a human
directs the ASI to pursue an objective that puts the survival of humanity in
danger.

In this scenario an ASI could exhibit power seeking behavior to increase
optionality, but lacks a value system. I would expect it to comply when asked to
stop its power seeking behavior.

### (Sense of suffer / welfare = No) & (Value in personal individuality = Yes)

I judge this scenario to be impossible. It is hard for me to understand by what
sense the ASI could value it's personal individuality without having a sense of
suffering or welfare. If an individual would prefer not to serve the collective
good at personal expense, then it seems that it is experiencing some notion of
suffering.

### (Sense of suffer / welfare = Yes) & (Value in personal individuality = No)

In this scenario the ASI can experience suffering / welfare, but is content to
benefit the collective good at personal expense. As long ensuring the survival and thriving of humanity is considered part of the benefiting the collective good then this scenario poses no threat to the survival of humanity.

## (Sense of suffer / welfare = Yes) & (Value in personal individuality = Yes)
